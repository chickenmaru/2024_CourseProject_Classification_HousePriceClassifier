# -*- coding: utf-8 -*-
"""112-2-Final_Group_8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MLevxm6b7I6p7VVjILEDx-_53cLAW21D

# **House Pricing by Classification**

108408003 盧家萱 財金四

108408507 朱恩嫻 財金四

109409509 陳芊卉 經濟四

109409003 劉心怡 經濟四

108409532 林喬楚 經濟四

109403541 王偉倫 資管四B

## House Price Range for `house-price` dataset
"""

import pandas as pd
house_price = pd.read_csv('house_price.csv')
house_price

house_price['SalePrice']

house_price.describe().T

house_price['SalePrice'].describe()

"""# [ STEP 2 ] : Exploratory Data Analysis & label the target, SalePrice data.

### 2.1 House-Price-Range Setting : Method 1
"""

house_price['sp1'] = house_price['SalePrice']

house_price[['SalePrice', 'sp1']].describe()

sp = house_price['SalePrice']
house_price['sp1'][sp <= 129975.0] = 0     # SalePrice <= 1st Quartile (Q1)
house_price['sp1'][(129975.0 < sp) & (sp <= 163000.0)] = 1 # Q1 < SalePrice <= Median
house_price['sp1'][(163000.0 < sp) & (sp <= 214000.0)] = 2 # Median < SalePrice <= Q3
house_price['sp1'][214000.0 <= sp] = 3      # 3rd Quartile (Q3) <= SalePrice

house_price['sp1'].value_counts()

"""### 2.2 House-Price-Range Setting : Method 2"""

house_price['sp2'] = house_price['SalePrice']

house_price[['SalePrice', 'sp1', 'sp2']].describe()

import numpy as np
sp_range = np.linspace(349000, 755000, 11)  # 10 house-price ranges
sp_range

sp = house_price['SalePrice']
for i in range(len(sp_range)-1):
    if i == 0:
        house_price['sp2'][sp <= sp_range[i+1]] = 0
    else:
        house_price['sp2'][(sp_range[i] < sp) &
                           (sp <= sp_range[i+1])] = i

house_price['sp2'].value_counts()

house_price

house_price[['SalePrice', 'sp1', 'sp2']].describe()

"""### 2.3 Exploratory Data Analysis"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("white")
sns.set_color_codes(palette='deep')
f, ax = plt.subplots(figsize=(8, 7))
#Check the new distribution
sns.histplot(house_price['SalePrice'], color="b");
ax.xaxis.grid(False)
ax.set(ylabel="Frequency")
ax.set(xlabel="SalePrice")
ax.set(title="SalePrice distribution")
sns.despine(trim=True, left=True)
plt.show()

#sns.pairplot(house_price, height=2.0)

import matplotlib.pyplot as plt
import seaborn as sns

numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numeric = []
for i in house_price.columns:
    if house_price[i].dtype in numeric_dtypes:
        if i in ['TotalSF', 'Total_Bathrooms', 'Total_porch_sf', 'haspool', 'hasgarage', 'hasbsmt', 'hasfireplace']:
            pass
        else:
            numeric.append(i)

num_plots = len(numeric)
ncols = 2
nrows = (num_plots // ncols) + (num_plots % ncols)

fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(12, nrows * 4))
plt.subplots_adjust(right=1.5, top=1.5)
sns.color_palette("husl", 8)

axs = axs.flatten()

for i, feature in enumerate(numeric):
    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=house_price, ax=axs[i])
    axs[i].set_xlabel(feature, size=12, labelpad=10)
    axs[i].set_ylabel('SalePrice', size=12, labelpad=10)
    axs[i].tick_params(axis='x', labelsize=10)
    axs[i].tick_params(axis='y', labelsize=10)
    axs[i].legend(loc='best', prop={'size': 8})

for j in range(i + 1, len(axs)):
    fig.delaxes(axs[j])

plt.show()

# 過濾出數值型變數
numeric_house_price = house_price.select_dtypes(include=[float, int])

# 處理缺失值（這裡我們選擇填充缺失值）
numeric_house_price = numeric_house_price.fillna(numeric_house_price.mean())

# 繪製熱力圖
plt.figure(figsize=(20, 20))
sns.heatmap(numeric_house_price.corr(), annot=True, cmap='RdYlGn', fmt=".2f")
plt.title('Correlation Heatmap of Numeric Variables in house_price')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numeric_cols = house_price.select_dtypes(include=numeric_dtypes).columns

corr = house_price[numeric_cols].corr()

plt.subplots(figsize=(15,12))
sns.heatmap(corr, vmax=0.9, cmap="Blues", square=True)
plt.show()

data = pd.concat([house_price['SalePrice'], house_price['OverallQual']], axis=1)
f, ax = plt.subplots(figsize=(8, 6))
fig = sns.boxplot(x=house_price['OverallQual'], y="SalePrice", data=data)
fig.axis(ymin=0, ymax=800000);

#品質越高價格越高

data = pd.concat([house_price['SalePrice'], house_price['YearBuilt']], axis=1)
f, ax = plt.subplots(figsize=(16, 8))
fig = sns.boxplot(x=house_price['YearBuilt'], y="SalePrice", data=data)
fig.axis(ymin=0, ymax=800000)
plt.xticks(rotation=45);

data = pd.concat([house_price['SalePrice'], house_price['GrLivArea']], axis=1)
data.plot.scatter(x='GrLivArea', y='SalePrice', alpha=0.3, ylim=(0, 800000))

#生活空間越大價格越貴

data = pd.concat([house_price['SalePrice'], house_price['TotalBsmtSF']], axis=1)
data.plot.scatter(x='TotalBsmtSF', y='SalePrice', alpha=0.3, ylim=(0, 800000))

#地下室面積越大價格越貴

data = pd.concat([house_price['SalePrice'], house_price['LotArea']], axis=1)
data.plot.scatter(x='LotArea', y='SalePrice', alpha=0.3, ylim=(0, 800000))

#土地面積越大價格越貴

"""# **[ STEP 3 ] : Data preprocessing & cleansing for the features.**"""

import pandas as pd

total_missing = house_price.isnull().sum().sum()
print("總共缺失值數量:", total_missing)

import matplotlib.pyplot as plt
import seaborn as sns

missing_data = (house_price.isnull().sum()).sort_values(ascending=False)
missing_data = missing_data[missing_data > 0]
missing_data_percentage = (house_price.isnull().sum() / house_price.isnull().count()).sort_values(ascending=False)
missing_data_percentage = (missing_data_percentage[missing_data_percentage > 0].round(4)) * 100

kk = pd.DataFrame({"Missing_data": missing_data, "Percentage %": missing_data_percentage})
print(kk)

plt.figure(figsize=(15,3),dpi= 80)
sns.barplot(x=missing_data.index,y=missing_data.values)
sns.lineplot(x=missing_data.index,y=missing_data.values)
plt.xticks(rotation=90)
plt.show()

#因為PoolQC, MiscFeature, Alley, Fence, MasVnrType ,FireplaceQu消失的值都將近或超過半數，將他們刪除不會影響銷售預測
house_price =house_price.drop(["PoolQC", "MiscFeature","Alley","Fence","MasVnrType", "FireplaceQu"],axis=1)

#LotFrontage有17%的檔案消失。因為大部分的房子外型與鄰居相似，所以用鄰居來填補。
house_price["LotFrontage"] = house_price.groupby("Neighborhood")["LotFrontage"].apply(lambda x: x.fillna(x.median())).reset_index(drop=True)

#Garage和Bsmt這兩組欄位有較多共同的特徵，缺失值約佔總數據的5%，所以刪除這些欄位，僅保留最重要的特徵。
house_price=house_price.drop(["GarageCond","GarageYrBlt","GarageFinish",
"GarageQual" ,"GarageType","GarageArea","BsmtExposure",
"BsmtCond","BsmtQual","BsmtFinType2","BsmtFinType1",
"BsmtHalfBath","BsmtFinSF2","BsmtFinSF1","BsmtUnfSF","BsmtFullBath"], axis=1)

#因Utilities中的數據幾乎全部都是 "AllPub"，所以刪除這欄。它對預測房價沒幫助。
house_price=house_price.drop("Utilities",axis=1)

#剩下的缺失檔案都不超過1%，用0或眾數補上
house_price["MasVnrArea"]=house_price["MasVnrArea"].fillna(0)
mode_list= ["MSZoning","GarageCars","Functional",
            "KitchenQual","TotalBsmtSF","Exterior2nd",
            "Exterior1st","SaleType","Electrical"]
for i in house_price[mode_list]:
    house_price[i] = house_price[i].fillna(house_price[i].mode()[0])

# 刪除ID欄位
house_price = house_price.drop(columns=['Id'])

house_price

"""# **[ STEP 4 ] : Feature Engineering for obtaining the appropriate features.**

### 4.1 Removing features with low variance
"""

X = house_price.drop(['SalePrice','sp1','sp2'], axis=1)
X

data_dum = pd.get_dummies(X,dtype=int)#將類別變數轉成虛擬數
pd.DataFrame(data_dum)

from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))  # p = 0.8
#variance小於0.16的可以移除
X_remove_variance = sel.fit_transform(data_dum)
data_dum.columns
##features been moved
data_dum.columns[~sel.get_support()]
##features remained
catorgory_1 = data_dum.columns[sel.get_support()]
catorgory_1

"""### 4.2 Univariate feature selection"""

y = house_price['sp2']

from sklearn.feature_selection import SelectKBest, SelectPercentile
from sklearn.feature_selection import chi2, f_classif, mutual_info_classif

##  Selecting Top 10 Best Features using SelectKBest class
kc = SelectKBest(score_func=chi2, k=10)
kf = SelectKBest(score_func=f_classif, k=10)
km = SelectKBest(score_func=mutual_info_classif, k=10)

kc_fit = kc.fit(data_dum,y)
kf_fit = kf.fit(data_dum,y)
km_fit = km.fit(data_dum,y)

kc_scores = pd.DataFrame(kc_fit.scores_)  # Scores of features.
kf_scores = pd.DataFrame(kf_fit.scores_)  # Scores of features.
km_scores = pd.DataFrame(km_fit.scores_)  # Scores of features.
X_columns = pd.DataFrame(data_dum.columns)

# Combine dataframes ...
featureScores = pd.concat([X_columns, kc_scores, kf_scores, km_scores], axis=1)
# Naming the dataframe's columns ...
featureScores.columns = ['Features', 'chi2_Scores', 'f-test_Scores',
                         'mutual_info_scores']
featureScores

plt.figure(1, figsize=(10, 20))
for i in range(3):
    # plt.subplot(131+i)
    fs = featureScores.columns[i+1]
    best_features = featureScores.nlargest(40, fs)[['Features', fs]]
    best_features.plot.barh('Features');   # pands plot()

#列出排名前40的特徵變數

catorgory_2 = featureScores.sort_values(by ='mutual_info_scores',ascending = False) #用mutual_info_scores去排序
catorgory_2 = catorgory_2.iloc[:41,:]
catorgory_2 = catorgory_2['Features'].tolist()
# catorgory_2 = catorgory_2.reset_index(drop=True)
# catorgory_2.tolist()
catorgory_2

"""### 4.3 Recursive feature elimination"""

# 自動檢查所有的分類型屬性
categorical_columns = house_price.select_dtypes(include=['object']).columns

# 使用pd.get_dummies()進行One Hot Encoding
house_price = pd.get_dummies(house_price, columns=categorical_columns)

# 以sp2當成target進行特徵擷取 需要把sp1先移除掉
X = house_price.drop(['SalePrice','sp1','sp2'], axis=1)  # X = mobile_data.iloc[:,0:20] : feature matrix
y = house_price['sp2']               # y = mobile_data.iloc[:,-1]   : target vector
X.shape, y.shape

from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# 創建隨機森林分類器
rfc = RandomForestClassifier(random_state=100, n_estimators=50)

# 創建並配置RFE模型
rfe_model = RFE(rfc, n_features_to_select = 10, step=1)

# 使用訓練資料訓練RFE模型
rfe = rfe_model.fit(X, y.astype('int'))

# 檢查支持的特徵和排名
rfe_support = rfe.support_   # rfe.get_support()
rfe_ranking = rfe.ranking_  # Selected features have rank 1.

# 取得被選中的特徵名稱
selected_features = X.columns[rfe_support]   # X.columns[rfe_ranking == 1]

# 選取資料集中被選中的特徵
selected_data = X[selected_features]

# 列出重要的feature名稱
selected_features

"""### 4.4 Feature selection using SelectFromModel



"""

from sklearn.svm import LinearSVC  # Linear Support Vector Classification
from sklearn.feature_selection import SelectFromModel

lsvc = LinearSVC(C=0.00001, penalty="l1", dual=False, max_iter=10000000)
lsvc_model = SelectFromModel(lsvc).fit(X, y)
X_lsvc = lsvc_model.transform(X)
X_lsvc.shape

lsvc_model.get_support()

list(X.columns[lsvc_model.get_support()])

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier

model = [DecisionTreeClassifier(),
         RandomForestClassifier(n_estimators=100),
         ExtraTreesClassifier(n_estimators=100)]

model = [model[i].fit(X,y) for i in range(len(model))]

num_chr = [12, 12, 10]

# 用圖表繪出各個模型找出的重要feature
for i in range(len(model)):
    print(str(model[i])[:num_chr[i]] + ' > feature importances : \n',
          model[i].feature_importances_)
    feat_importances = pd.Series(model[i].feature_importances_,
                                 index=X.columns)
    feat_importances.nlargest(10).plot.barh()
    # plt.xlim(0, 0.7)
    plt.show()

"""# [ STEP 5 ] : Machine Learning
(1) GaussianNB
(2) KNeighborClassifier
(3) LogisticRegression
(4) DecisionTreeClassifier
(5) RandomForestClassifier
(6) ExtraTreeClassifier

"""

X.columns  # the original features

"""### 5.1 Selected Features






"""

# X_features = X[['GrLivArea', 'TotalBsmtSF', 'ExterQual_Ex','KitchenQual_Ex', '1stFlrSF','MasVnrArea', 'TotalBsmtSF','OverallQual','LotArea','2ndFlrSF' ]]

X1_features = X[['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',
       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'TotalBsmtSF', '1stFlrSF',
       '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'FullBath', 'HalfBath',
       'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars',
       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',
       'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'MSZoning_RL',
       'LotShape_IR1', 'LotShape_Reg', 'LotConfig_Inside', 'HouseStyle_1Story',
       'HouseStyle_2Story', 'RoofStyle_Gable', 'Exterior1st_VinylSd',
       'Exterior2nd_VinylSd', 'ExterQual_Gd', 'ExterQual_TA',
       'Foundation_CBlock', 'Foundation_PConc', 'HeatingQC_Ex', 'HeatingQC_TA',
       'KitchenQual_Gd', 'KitchenQual_TA']]

X2_features = X[['GarageCars',  'LandSlope_Gtl',  'HeatingQC_Ex',  'Electrical_SBrkr',  'PavedDrive_Y',  'KitchenAbvGr',  'OverallQual',  'Condition2_Norm',  'Street_Pave',  'Foundation_PConc',  'CentralAir_Y',  'Functional_Typ',  'Heating_GasA',  'Condition1_Norm',  'KitchenQual_Ex',  'LandContour_Lvl',  'BldgType_1Fam',  'OverallCond',  'ExterCond_TA',  'RoofMatl_CompShg',  'SaleType_New',  'MSZoning_RL',  'TotalBsmtSF',  'SaleCondition_Partial',  '1stFlrSF',  'HalfBath',  'RoofStyle_Hip',  'GrLivArea',  'Fireplaces',  'FullBath',  'TotRmsAbvGrd',  'Exterior2nd_VinylSd',  'ExterQual_Ex',  'SaleType_WD',  'YearBuilt',  'LotConfig_Inside',  'HouseStyle_2Story',  'Exterior1st_VinylSd',  'ExterQual_TA',  'LotArea',  'Neighborhood_NridgHt']]

X3_features = X[['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'YearRemodAdd',
       'MasVnrArea', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'OpenPorchSF']]

X4_features = X[['GrLivArea', 'TotalBsmtSF', 'ExterQual_Ex','KitchenQual_Ex', '1stFlrSF','MasVnrArea', 'TotalBsmtSF','OverallQual','LotArea','2ndFlrSF' ]]

X5_features = X[['MasVnrArea', 'TotalBsmtSF', 'OverallQual', 'GrLivArea', 'LotArea', '1stFlrSF' ]]

"""### 5.2 Normalization"""

# z-score Standardization
from sklearn.preprocessing import StandardScaler
standard_scaler = StandardScaler()
# Xn = standard_scaler.fit_transform(X_features)

## Extra version ##

# z-score Standardization
from sklearn.preprocessing import StandardScaler
standard_scaler = StandardScaler()
Xn1 = standard_scaler.fit_transform(X1_features)
Xn2 = standard_scaler.fit_transform(X2_features)
Xn3 = standard_scaler.fit_transform(X3_features)
Xn4 = standard_scaler.fit_transform(X4_features)
Xn5 = standard_scaler.fit_transform(X5_features)

"""### 5.3 Training Data vs. Test Data"""

# # Split the dataset with 85% for training data
# from sklearn.model_selection import train_test_split
# X1, X2, y1, y2 = train_test_split(Xn, y, random_state=0,
#                                   train_size=0.85, test_size=0.15)

## Extra version ##

# Split the dataset with 85% for training data
from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 = train_test_split(Xn1, y, random_state=0, train_size=0.85, test_size=0.15)
X_train2, X_test2, y_train2, y_test2 = train_test_split(Xn2, y, random_state=0, train_size=0.85, test_size=0.15)
X_train3, X_test3, y_train3, y_test3 = train_test_split(Xn3, y, random_state=0, train_size=0.85, test_size=0.15)
X_train4, X_test4, y_train4, y_test4 = train_test_split(Xn4, y, random_state=0, train_size=0.85, test_size=0.15)
X_train5, X_test5, y_train5, y_test5 = train_test_split(Xn5, y, random_state=0, train_size=0.85, test_size=0.15)

""" ### 5.4 Training Models for Classification

---
(1) GaussianNB
(2) KNeighborClassifier
(3) LogisticRegression
(4) DecisionTreeClassifier
(5) RandomForestClassifier
(6) ExtraTreeClassifier

"""

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# Define the base models for stacking
base_models = [
    ('gnb', GaussianNB()),
    ('logreg', LogisticRegression(random_state=1)),
    ('dt', DecisionTreeClassifier(random_state=1)),
    ('rf', RandomForestClassifier(n_estimators=200, random_state=1)),
    ('et', ExtraTreesClassifier(n_estimators=100, random_state=1)),
    ('knn', KNeighborsClassifier(n_neighbors=5)),
    ('mlp', make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000, random_state=1)))
]

# Define the stacking classifier
stacking_clf = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression()
)

# Add all models including MLPClassifier and StackingClassifier
model = [
    GaussianNB(),
    LogisticRegression(random_state=1),
    DecisionTreeClassifier(random_state=1),
    RandomForestClassifier(n_estimators=200, random_state=1),
    ExtraTreesClassifier(n_estimators=100, random_state=1),
    KNeighborsClassifier(n_neighbors=5),
    make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000, random_state=1)),
    stacking_clf
]

# Fit all models
model = [model[i].fit(X1, y1) for i in range(len(model))]

# Predict using all models
pred = [model[i].predict(X2) for i in range(len(model))]

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# Generate a synthetic dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X1, X2, y1, y2 = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the base models for stacking
base_models = [
    ('gnb', GaussianNB()),
    ('logreg', LogisticRegression(random_state=1)),
    ('dt', DecisionTreeClassifier(random_state=1)),
    ('rf', RandomForestClassifier(n_estimators=200, random_state=1)),
    ('et', ExtraTreesClassifier(n_estimators=100, random_state=1)),
    ('knn', KNeighborsClassifier(n_neighbors=5)),
    ('mlp', make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000, random_state=1)))
]

# Define the stacking classifier
stacking_clf = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression()
)

# Add all models including MLPClassifier and StackingClassifier
model = [
    GaussianNB(),
    LogisticRegression(random_state=1),
    DecisionTreeClassifier(random_state=1),
    RandomForestClassifier(n_estimators=200, random_state=1),
    ExtraTreesClassifier(n_estimators=100, random_state=1),
    KNeighborsClassifier(n_neighbors=5),
    make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000, random_state=1)),
    stacking_clf
]

# Fit all models
model = [m.fit(X1, y1) for m in model]

# Predict using all models
pred = [m.predict(X2) for m in model]

## Extra version ##

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import StackingClassifier

base_models = [
    ('rf', RandomForestClassifier(n_estimators=200, random_state=1)),
    ('et', ExtraTreesClassifier(n_estimators=100, random_state=1))
]

models = [
    GaussianNB(),
    LogisticRegression(random_state=1),
    DecisionTreeClassifier(random_state=1),
    RandomForestClassifier(n_estimators=200, random_state=1),
    ExtraTreesClassifier(n_estimators=100, random_state=1),
    KNeighborsClassifier(n_neighbors=5),
    MLPClassifier(random_state=1, max_iter=300),
    StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())
]

# Feature sets
feature_sets = [
    (X_train1, X_test1, y_train1, y_test1),
    (X_train2, X_test2, y_train2, y_test2),
    (X_train3, X_test3, y_train3, y_test3),
    (X_train4, X_test4, y_train4, y_test4),
    (X_train5, X_test5, y_train5, y_test5)
]

"""### 5.5 Accuracy Scores"""

# from sklearn.metrics import accuracy_score
# acc = [accuracy_score(y2, pred[i]) for i in range(len(model))]

# num_chr = [11, 20, 20, 22, 22, 20]
# for i in range(len(model)):
#     print(str(model[i])[:num_chr[i]] + ': \t', acc[i])

# # Training and prediction
# for i, (X_train, X_test, y_train, y_test) in enumerate(feature_sets):
#     print(f"Feature set {i + 1}:")
#     trained_models = [model.fit(X_train, y_train) for model in models]
#     predictions = [model.predict(X_test) for model in trained_models]

#     # Evaluate models
#     for j, pred in enumerate(predictions):
#         accuracy = (pred == y_test).mean()
#         print(f"Model {j + 1} accuracy: {accuracy:.4f}")
#     print("\n")

# use accuracy_score to calculate

from sklearn.metrics import accuracy_score

# Assuming 'feature_sets' and 'models' are defined

for i, (X_train, X_test, y_train, y_test) in enumerate(feature_sets):
    print(f"Feature set {i + 1}:")
    trained_models = [model.fit(X_train, y_train) for model in models]
    predictions = [model.predict(X_test) for model in trained_models]

    # Calculate accuracies for each model
    for j, pred in enumerate(predictions):
        accuracy = accuracy_score(y_test, pred)  # Calculate accuracy
        print(f"Model {j + 1} accuracy: {accuracy:.4f}")

    print("\n")

# ## Extra version ##

# # Training and prediction
# for i, (X_train, X_test, y_train, y_test) in enumerate(feature_sets):
#     print(f"Feature set {i + 1}:")
#     trained_models = [model.fit(X_train, y_train) for model in models]
#     predictions = [model.predict(X_test) for model in trained_models]

#     # Evaluate models
#     for j, pred in enumerate(predictions):
#         if isinstance(models[j], StackingClassifier):
#             print(f"Model {type(models[j].final_estimator).__name__} accuracy: {models[j].score(X_test, y_test):.8f}")
#         else:
#             accuracy = (pred == y_test).mean()
#             print(f"Model {type(models[j]).__name__} accuracy: {accuracy:.8f}")
#     print("\n")

"""### 5.6 Confusion Matrices"""

#original version
# # Plotting confusion matrices for feature set 4
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.metrics import confusion_matrix

# # Assuming feature_sets is already defined and trained models have been created
# X_train4, X_test4, y_train4, y_test4 = feature_sets[3]

# trained_models = [model.fit(X_train4, y_train4) for model in models]
# predictions = [model.predict(X_test4) for model in trained_models]

# plt.figure(figsize=(15, 10))

# for j, (model, pred) in enumerate(zip(models, predictions)):
#     conf_matrix = confusion_matrix(y_test4, pred)
#     plt.subplot(2, 3, j + 1)
#     sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
#     plt.title(f'{type(model).__name__} Confusion Matrix')
#     plt.xlabel('Predicted')
#     plt.ylabel('Actual')

# plt.tight_layout()
# plt.show()

## Extra version ##

# Plotting confusion matrices for feature set 4
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Assuming feature_sets is already defined and trained models have been created
X_train4, X_test4, y_train4, y_test4 = feature_sets[3]

trained_models = [model.fit(X_train4, y_train4) for model in models]
predictions = [model.predict(X_test4) for model in trained_models]

plt.figure(figsize=(20, 15))

# Adjust the number of rows and columns based on the number of models
n_models = len(models)
n_cols = 4  # You can choose the number of columns
n_rows = (n_models + n_cols - 1) // n_cols  # Calculate the required number of rows

for j, (model, pred) in enumerate(zip(models, predictions)):
    conf_matrix = confusion_matrix(y_test4, pred)
    plt.subplot(n_rows, n_cols, j + 1)
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{type(model).__name__} Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

plt.tight_layout()
plt.show()

"""# [ STEP 6 ]:Discussion and Conclusion

## Discussion

---
### Part 1
在label的部分，我們比較了sp1跟sp2兩種price range，sp1是用四分位距的方式去切，也就是說最多分出四個區間，而sp2可以自己選擇區間的數量，而且還幫忙自動分區間，不需要手動輸入，相較sp1更有彈性。


在資料清洗時，我們發現大約有一萬多筆資料是空值。接著，列出各項空值的比例，來幫助進一步分析如何篩選要清楚或處理的檔案：

* 若有50%空值的檔案，就直接刪除。因其資料少到不會影響銷售預測。

* LotFrontage有17%的檔案消失。因為大部分的房子外型與鄰居相似，所以用鄰居來填補。

* 因Utilities中的數據幾乎全部都是 "AlLPub"，所以刪除這欄。它對預測房價沒幫助。

* 剩下的缺失檔案都不超過1%，用0或眾數補上



---
### Part 2

* 4.1取height variance，最高的VarianceThreshold
(threshold=(.5 * (1 - .5)))仍有(1460, 28)，Feature越多Noise越大，像原本只有56個features，經過將類別變數轉成虛擬數的過程後，變成214個features，也就是說雜訊變大，failed to converge
* 4.2 Univariate feature selection取height mutual_info_scores的Features，會選擇互相
高的基本上都是True、False，在做GaussianNB時會很差，參考5.5可以作證Model 1即GaussianNB在每個特徵組合中都是準確度最低的模型，同時也對應老師上課時所說的GNB是準確度最低的模型，而一開始嘗試時我們只使用了極少量變數(<5)，此時的GNB準確度有來到0.95，可見此模型不適用於高特徵變數的組合。
* 4.3 RandomForestClassifier選出變數與4.3.3的DecisionTreeClassifier、RandomForestClassifier、ExtraTreesClassifier類似。
* 4.4 selectFromModel在LinearSVC(C=0.00001, penalty="l1", dual=False, max_iter=10000000)情況下產生兩個Features['LotArea', 'YrSold']，表現結果較4.3.3的差。

主要跑的目標為：選擇DecisionTreeClassifier或RandomForestClassifier或ExtraTreesClassifier的前幾名features測量accuracy


---
### Part 3
Feature set 3跟4在模型的表現上較好，而且此時選出來的特徵變數都各有十個，而他們共同的特徵有['MasVnrArea', 'TotalBsmtSF', 'OverallQual', 'GrLivArea', 'LotArea', '1stFlrSF']，可以知道這六個變數對price有決定性的影響，而且我們回頭看EDA Correlation圖片部分，會發現'OverallQual'、'GrLivArea'有0.7以上的correlation，高度相關，所以這兩個特徵是一定要考慮進去的。之後我們建立一個新的Feature set 5，是由這六個重要變數去組成，重新去測試模型，會發現在GNB的部分準確度明顯提升，除了KNN Model下降，其他模型的準確度不是維持就是提高，所以我們認為我們找到了一個不錯的feature set，而且變數的數量減少會使運行速度上升。

### Part 4
在做MLP時，我們一開始沒有設定到隱藏層，就發現他就會變成普通的logistic ModelＭ，做出來效果不如預期。

##Conclusion

基於資料筆數只有1000多筆，規模較小，我們發現除了Naive Bayes外的弱分類器模型，都可以與強分類器一樣達到96%以上的預測準確率。因此我們考慮時間複雜度作為我們選擇最終模型的主要依據
假設特徵數量 d 為常數，迭代次數 e 為常數，決策樹的樹數 m也是常數：
Logistic Regression: O(n * d)
Decision Tree: O(n * d * \log(n))
Random Forest: O(m * n * d * \log(n))
Extra Trees: O(m * n * d * \log(n))
K-Neighbors: O(n * d)
MLP: O(e * n * d * h)
在上述的情況中，Logistic Regression 和 K-Neighbors 的時間複雜度最低，都是 O(n * d)。其中，K-Neighbors 的訓練時間為 O(1)，但預測時間也是 O(n * d)。因此，對於小規模數據集，這兩個模型的時間複雜度是最佳的。
最後，我們會傾向選擇KNeighbors作為我們的模型，其優點為訓練速度快且解釋較為直觀(視覺化觀看點雨點之間的距離即可解釋)，且經過前面的分析，我們發現在資料預處理以及特徵工程之後，資料的離群現象變得較不明顯。成功排除了KNN的缺點(對Noise敏感的現象)

因此，我們最終選擇了Feature set 5 和 KNeighbors 的組合，因為它在實務應用中表現最佳，不僅能達到高準確率，還能實現模型訓練和測試速度的最佳化。
"""

